{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Feature Extraction\n",
    "\n",
    "Models (with tuned hyperparameters) are compared on a classification problem. Final extracted features are computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-computed features\n",
    "The six outputs of preprocessing are imported:\n",
    "1. 'data/prepped.pkl' is a pickled dataframe containing cleaned comments and their subreddit label\n",
    "2. 'data/feature_names.pkl' is a pickled series containing the words that were counted\n",
    "3. 'data/term_counts.npz' is a scipy sparse matrix containing the word counts\n",
    "4. 'data/tf.npz' is a scipy sparse matrix containing the word frequencies\n",
    "5. 'data/tf.npz' is a scipy sparse matrix containing the word frequencies weighted by their inverse document frequency\n",
    "6. 'data/gensim_sentences.pkl' is a list of gensim labelled sentence objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sfzba\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "done in 37.981s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from scipy.sparse import load_npz\n",
    "import pickle\n",
    "from gensim.utils import unpickle as gensim_unpickle\n",
    "\n",
    "print(\"Importing data...\")\n",
    "t0 = time()\n",
    "data = pd.read_pickle('data/prepped.pkl')\n",
    "with open('data/feature_names.pkl', 'rb') as fp:\n",
    "    feature_names = pickle.load(fp)\n",
    "cts = load_npz('data/term_counts.npz')\n",
    "tf = load_npz('data/tf.npz')\n",
    "tfidf = load_npz('data/tfidf.npz')\n",
    "sentences = gensim_unpickle('data/gensim_sentences.pkl')\n",
    "print(\"done in %0.3fs\" % (time()-t0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "In order to compare feature extraction methods using a gold standard, we formulate a classification problem against each comments' subreddit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "Twenty percent of the data is left out for cross-validation, with class balances kept intact as best as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X = data['body'].astype('unicode')\n",
    "# X = cts\n",
    "# X = tf\n",
    "X = tfidf\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(data.subreddit)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2, shuffle=True, stratify=y)\n",
    "sents_train, sents_test = train_test_split(sentences, random_state=42, test_size=0.2, shuffle=True, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute word frequency matrix decompositions on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LSA model\n",
      "done in 67.320s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Global hyper-parameter\n",
    "n_components = 60\n",
    "\n",
    "print(\"Fitting LSA model\")\n",
    "t0 = time()\n",
    "LSA_model = TruncatedSVD(n_components=n_components, random_state=0).fit(X_train)\n",
    "LSA_train = LSA_model.transform(X_train)\n",
    "LSA_test  = LSA_model.transform(X_test)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pLSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting pLSA model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d23d31c901fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m pLSA_model = NMF(n_components=n_components, random_state=0,\n\u001b[0;32m      7\u001b[0m                  \u001b[0mbeta_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'kullback-leibler'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                  l1_ratio=.5).fit(X_train)\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mpLSA_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpLSA_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mpLSA_test\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mpLSA_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sfzba\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\nmf.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1258\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \"\"\"\n\u001b[1;32m-> 1260\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1261\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sfzba\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\nmf.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, W, H)\u001b[0m\n\u001b[0;32m   1233\u001b[0m             \u001b[0ml1_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'both'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m             \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m             shuffle=self.shuffle)\n\u001b[0m\u001b[0;32m   1236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m         self.reconstruction_err_ = _beta_divergence(X, W, H, self.beta_loss,\n",
      "\u001b[1;32mC:\\Users\\sfzba\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\nmf.py\u001b[0m in \u001b[0;36mnon_negative_factorization\u001b[1;34m(X, W, H, n_components, init, update_H, solver, beta_loss, tol, max_iter, alpha, l1_ratio, regularization, random_state, verbose, shuffle)\u001b[0m\n\u001b[0;32m   1026\u001b[0m                                                   \u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1_reg_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1_reg_H\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m                                                   \u001b[0ml2_reg_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_reg_H\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate_H\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1028\u001b[1;33m                                                   verbose)\n\u001b[0m\u001b[0;32m   1029\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sfzba\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\nmf.py\u001b[0m in \u001b[0;36m_fit_multiplicative_update\u001b[1;34m(X, W, H, beta_loss, max_iter, tol, l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H, update_H, verbose)\u001b[0m\n\u001b[0;32m    797\u001b[0m         \u001b[1;31m# test convergence criterion every 10 iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtol\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 799\u001b[1;33m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_beta_divergence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquare_root\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    800\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sfzba\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\nmf.py\u001b[0m in \u001b[0;36m_beta_divergence\u001b[1;34m(X, W, H, beta, square_root)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;31m# compute np.dot(W, H) only where X is nonzero\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0mWH_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_special_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[0mX_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sfzba\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\nmf.py\u001b[0m in \u001b[0;36m_special_sparse_dot\u001b[1;34m(W, H, X)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mii\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[0mdot_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m         \u001b[0mWH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdot_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mii\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mWH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "print(\"Fitting pLSA model\")\n",
    "# Use the non-negative matrix factorization with generalized Kullback-Leibler divergence\n",
    "t0 = time()\n",
    "pLSA_model = NMF(n_components=n_components, random_state=0,\n",
    "                 beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "                 l1_ratio=.5).fit(X_train)\n",
    "pLSA_train = pLSA_model.transform(X_train)\n",
    "pLSA_test  = pLSA_model.transform(X_test)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-8228c51bd2da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m LDA_model = LatentDirichletAllocation(n_components=n_components, learning_decay=0.51, learning_offset=2,\n\u001b[1;32m----> 6\u001b[1;33m                                       learning_method='online', random_state=0).fit(X_train)\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mLDA_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLDA_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mLDA_test\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mLDA_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\sfzba\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    550\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0midx_slice\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m                         self._em_step(X[idx_slice, :], total_samples=n_samples,\n\u001b[1;32m--> 552\u001b[1;33m                                       batch_update=False, parallel=parallel)\n\u001b[0m\u001b[0;32m    553\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m                     \u001b[1;31m# batch update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "print(\"Fitting LDA model\")\n",
    "t0 = time()\n",
    "LDA_model = LatentDirichletAllocation(n_components=n_components, learning_decay=0.51, learning_offset=2,\n",
    "                                      learning_method='online', random_state=0).fit(X_train)\n",
    "LDA_train = LDA_model.transform(X_train)\n",
    "LDA_test  = LDA_model.transform(X_test)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "\n",
    "print(\"Training doc2vec\")\n",
    "model = Doc2Vec(size=n_components, iter=75, dm=1, min_count=2)\n",
    "model.build_vocab(sents_train)\n",
    "model.train(sents_train, total_examples=model.corpus_count, epochs=model.iter)\n",
    "\n",
    "doc2vec_train = np.empty([model.docvecs.count, n_components])\n",
    "for i in range(model.docvecs.count):\n",
    "    doc2vec_train[i,:] = model.docvecs[i]\n",
    "\n",
    "doc2vec_test = np.empty([len(sents_test), n_components])\n",
    "for i, vec in enumerate(sents_test):\n",
    "    doc2vec_test[i,:] = model.infer_vector(vec.words)\n",
    "\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save outputs to disk in case of computation interruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extractors = [\n",
    "#     (\"LSA\", LSA_train, LSA_test),\n",
    "#     (\"pLSA\", pLSA_train, pLSA_test),\n",
    "#     (\"LDA\", LDA_train, LDA_test),\n",
    "#     (\"doc2vec\", doc2vec_train, doc2vec_test),\n",
    "# ]\n",
    "# with open('data/temp_extractors.pkl', 'wb') as fp:\n",
    "#     pickle.dump(extractors, fp)\n",
    "\n",
    "with open('data/temp_extractors.pkl', 'rb') as fp:\n",
    "    extractors = pickle.load(fp)\n",
    "# _, LSA_train, LSA_test = extractors[0]\n",
    "# _, pLSA_train, pLSA_test = extractors[1]\n",
    "# _, LDA_train, LDA_test = extractors[2]\n",
    "# _, doc2vec_train, doc2vec_test = extractors[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(362517, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractors[3][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362517"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test feature extractors using various classifiers\n",
    "\n",
    "KNN is directly related to the method for subreddit recommendation. It is therefore a reasonable choice for classifier here.\n",
    "\n",
    "If desired, it may interesting to compare to fast, memory-efficient methods like SGD implementations of a linear SVM and logisitic regression, or a simple decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "extractors = [\n",
    "     (\"LSA\", LSA_train, LSA_test),\n",
    "     (\"pLSA\", pLSA_train, pLSA_test),\n",
    "     (\"LDA\", LDA_train, LDA_test),\n",
    "     (\"doc2vec\", doc2vec_train, doc2vec_test),\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "#    (\"Linear SVM\", SGDClassifier(loss='hinge', n_jobs=-1, random_state=42)),\n",
    "#    (\"Logistic Regression\", SGDClassifier(loss='log', n_jobs=-1, random_state=42)),\n",
    "    (\"Nearest Neighbors\", KNeighborsClassifier()),\n",
    "#    (\"Decision Tree\", DecisionTreeClassifier(max_depth=5)),\n",
    "]\n",
    "\n",
    "for ex_name, ex_train, ex_test in extractors:\n",
    "    print(\"%s\" % ex_name)\n",
    "    print()\n",
    "    for cl_name, clf in classifiers:\n",
    "        print(\"   %s\" % cl_name)\n",
    "        print()\n",
    "        \n",
    "        # F1 score undefined for any label with no predicted members\n",
    "        # Use accuracy instead\n",
    "        t0 = time()\n",
    "        clf.fit(ex_train, y_train)\n",
    "        t1 = time()\n",
    "        score = clf.score(ex_test, y_test) \n",
    "        t2 = time()\n",
    "        train_score = clf.score(ex_train, y_train)\n",
    "        \n",
    "        print(\"           Train\")\n",
    "        print(\"   Train time: %0.3f sec\" % (t1-t0))\n",
    "        print(\"   Accuracy  : %0.2f %%\" % (100*train_score))\n",
    "        print(\"   Acc. mult.: %0.2f\" % (train_score*(y.max()-y.min()+1)))\n",
    "        print(\"           Test\")\n",
    "        print(\"   Score time: %0.3f sec\" % (t2-t1))\n",
    "        print(\"   Accuracy  : %0.2f %%\" % (100*score))\n",
    "        print(\"   Acc. mult.: %0.2f\" % (score*(y.max()-y.min()+1)))\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "does not compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute decompositions at subreddit level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from scipy.sparse import load_npz\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.utils import unpickle as gensim_unpickle\n",
    "\n",
    "# cts = load_npz('data/term_counts_sub.npz')\n",
    "# tf = load_npz('data/tf_sub.npz')\n",
    "tfidf = load_npz('data/tfidf_sub.npz')\n",
    "sentences = gensim_unpickle('data/gensim_sentences_sub.pkl')\n",
    "\n",
    "X = tfidf\n",
    "\n",
    "n_components = 60\n",
    "\n",
    "print(\"LSA...\")\n",
    "t0 = time()\n",
    "LSA = TruncatedSVD(n_components=n_components, random_state=0).fit(X)\n",
    "X_LSA = LSA.transform(X)\n",
    "np.save('data/X_LSA_sub.npy', X_LSA)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"pLSA...\")\n",
    "t0 = time()\n",
    "pLSA = NMF(n_components=n_components, random_state=0,\n",
    "           beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "           l1_ratio=.5).fit(X)\n",
    "X_pLSA = pLSA.transform(X)\n",
    "np.save('data/X_pLSA_sub.npy', X_pLSA)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"LDA...\")\n",
    "t0 = time()\n",
    "LDA = LatentDirichletAllocation(n_components=n_components, learning_decay=0.51, learning_offset=2,\n",
    "                                learning_method='online', random_state=0).fit(X)\n",
    "X_LDA = LDA.transform(X)\n",
    "np.save('data/X_LDA_sub.npy', X_LDA)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"doc2vec...\")\n",
    "t0 = time()\n",
    "np.random.shuffle(sentences) # may improve accuracy\n",
    "model = Doc2Vec(size=n_components, iter=75, dm=1, min_count=2)\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "# Form numpy matrix from gensim array\n",
    "doc2vec = np.empty([model.docvecs.count, n_components])\n",
    "for i in range(model.docvecs.count):\n",
    "    doc2vec[i,:] = model.docvecs[i]\n",
    "np.save('data/doc2vec_sub.npy', doc2vec)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/extraction_models.pkl', 'rb') as fp:\n",
    "    pickle.dump((LSA, pLSA, LDA, doc2vec), fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations for word frequency matrix decompositions\n",
    "Each column of the output matrices can be seen as a topic, some weighted mixture of words. The most heavily weighted words for each topic are shown, either as a text list, a plot of their respective weights, or as a wordcloud (with size proportional to each words' weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_npz('data/tfidf.npz')\n",
    "print(\"LSA...\")\n",
    "t0 = time()\n",
    "LSA = TruncatedSVD(n_components=n_components, random_state=0).fit(X)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import pickle\n",
    "with open('data/feature_names.pkl', 'rb') as fp:\n",
    "    feature_names = pickle.load(fp)\n",
    "# with open('data/extraction_models.pkl', 'rb') as fp:\n",
    "#     LSA, pLSA, LDA, doc2vec = pickle.load(fp)\n",
    "n_top_words = 10\n",
    "\n",
    "fs = 18\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words):\n",
    "    #norm_comps = model.components_ / model.components_.sum(axis=1)[:, np.newaxis]\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        x = range(n_top_words,0,-1)\n",
    "        sort_idx = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        plt.figure(figsize=[6,6])\n",
    "        plt.barh(x, topic[sort_idx])\n",
    "        plt.yticks(x, np.asarray(feature_names)[sort_idx], fontsize=fs)\n",
    "        plt.title((\"Topic #%d: \" % topic_idx), fontsize=fs)\n",
    "        plt.xlabel(\"LSA Weight\", fontsize=fs)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def show_top_wc(model, feature_names, n_top_words):\n",
    "    wc = WordCloud(background_color=\"white\", max_words=n_top_words)\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d: \" % topic_idx)\n",
    "        words = {}\n",
    "        for i in topic.argsort()[:-n_top_words - 1:-1]:\n",
    "            words[feature_names[i]] = topic[i]\n",
    "        wc.generate_from_frequencies(words)\n",
    "        plt.imshow(wc, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\nTopics in LSA model:\")\n",
    "# print_top_words(LSA, feature_names, n_top_words)\n",
    "plot_top_words(LSA, feature_names, n_top_words)\n",
    "\n",
    "# print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "# # print_top_words(pLSA, feature_names, n_top_words)\n",
    "# plot_top_words(pLSA, feature_names, n_top_words)\n",
    "# \n",
    "# print(\"\\nTopics in LDA model:\")\n",
    "# # print_top_words(LDA, feature_names, n_top_words)\n",
    "# plot_top_words(LDA, feature_names, n_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
