{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL for Reddit Data\n",
    "\n",
    "This notebook extracts comments from the hottest posts in specified subreddits.\n",
    "\n",
    "Example of simple data exploration is shown with wordcloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "## reddit API wrapper (https://praw.readthedocs.io)\n",
    "import praw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set up reddit instance\n",
    "Requires Reddit API keys `client_id`, `client_secret`, and `user_agent` to be declared in the separate file \"secrets.py\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secrets import *\n",
    "\n",
    "reddit = praw.Reddit(client_id=CLIENT_ID,\n",
    "                     client_secret=CLIENT_SECRET,\n",
    "                     user_agent=USER_AGENT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Easy wrappers for praw\n",
    "\n",
    "Reddit is organized into a forest: The roots are called *subreddits*, to which users contribute *submissions*. Each submission has its own comment forest, a collection of comments on the submission itself or on other comments. At the submission level and below, Reddit sorts each submission or comment according to *hotness*, a measure of popularity. This organization is intuitive from the interactive frontend, but the API is a little confusing. Praw is the dominant Python wrapper for the Reddit API.\n",
    "\n",
    "Praw defines a `Reddit` instance, providing access to the site. From there we may request `Submissions` from any subreddit (or we can ask for 'all' to aggregate across all subreddits). These submissions come with their attached comment forests in the `comments` attribute. We can get a flattened list of all comments from the `list` method of `comments`.\n",
    "\n",
    "##### Pruning the Comment Forests\n",
    "Here's where things get tricky: Reddit prunes the comment forests, leaving `MoreComments` placeholders. We may request these pruned comments from Reddit using the `replace_more` method of the praw `Comment` object, up to *limit* comments. We may also set a *threshold* for how many children (including `MoreComments` placeholders) a comment must have to be included.\n",
    "\n",
    "This pruning is performed because, in practice, it is often the case that only the top few levels of comments are interesting, with the rest devolving into off-topic conversation. Praw does not offer an easy way to specify the maximum depth of the comment trees to be returned. The function `get_comments` implements a breadth-first search to enforce a hard maximum depth. When no maximum depth is specified, the only pruning done is by Reddit's `MoreComments` feature, and the breadth first search is equivalent to the list method of a praw `Submission`'s comment forest.\n",
    "\n",
    "Of course, flattening and pruning the tree loses information: it would be impossible to read a thread and understand the conversation. However, storing the comments in a tidy structure like a DataFrame makes analysis of the submission corpora much easier.\n",
    "\n",
    "##### Transforming reddit markdown\n",
    "The function `to_dataframe` strips leading/trailing whitespace and all newline characters. It also attempts to remove markdown metacharacters, as detailed [here](https://www.reddit.com/r/reddit.com/comments/6ewgt/reddit_markdown_primer_or_how_do_you_do_all_that/c03nik6/), to mixed success. In particular, it does not gracefully handle quotes/superscripts (which both use a carat >); carats are simply removed, sometimes joining superscript to its base. The function also simply leaves links alone, with format \\[text\\](link \"tooltip text\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_comments(submission, depth=None, limit=32, threshold=0):\n",
    "    '''Flatten comment forest up to specified depth after replacing 'MoreComments' instances.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    submission -- submission instance from praw\n",
    "    depth      -- maximum traversal depth of comment tree; None returns whole tree (default None)\n",
    "    limit      -- number of MoreComments instances to replace (default 32)\n",
    "    threshold  -- minimum number of children required to expand MoreComments (default 0)\n",
    "    '''\n",
    "    submission.comments.replace_more(limit=limit, threshold=threshold)\n",
    "    \n",
    "    if depth == None:\n",
    "        return submission.comments.list()\n",
    "    \n",
    "    comments = []\n",
    "    level = 0\n",
    "    sentinel = ['placeholder']\n",
    "    comment_queue = submission.comments[:]\n",
    "    comment_queue.extend(sentinel)\n",
    "    while comment_queue and level < depth:\n",
    "        comment = comment_queue.pop(0)\n",
    "        if [comment] == sentinel:\n",
    "            level = level + 1\n",
    "            comment_queue.extend(sentinel)\n",
    "            continue\n",
    "        comments.append(comment)\n",
    "        comment_queue.extend(comment.replies)\n",
    "    return comments\n",
    "\n",
    "def to_dataframe(comments, columns=['author', 'created_utc']):\n",
    "    '''Transform comments list into pandas dataframe. Strip newlines and leading/trailing whitespace.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    comments -- list of praw Comment instances\n",
    "    columns  -- list of column names corresponding to praw Comment attributes (default ['author', 'created_utc'])\n",
    "    '''\n",
    "    data = pd.DataFrame(data=[comment.body for comment in comments],columns=['body'])\n",
    "    for col in columns:\n",
    "        data[col] = np.array([getattr(comment,col) for comment in comments])\n",
    "        \n",
    "    for index, row in data.iterrows():\n",
    "        line = row.body\n",
    "        line = re.sub('[\\\\n]', ' ', line)\n",
    "        line = re.sub('~~|\\*|_|#|\\`|\\>', '', line)\n",
    "        line = line.strip()\n",
    "        data.loc[index,'body'] = line\n",
    "    return data\n",
    "\n",
    "def load_data(reddit, subreddit='all', limit_subs=10, depth=None, limit_coms=32, threshold=0):\n",
    "    '''Extract, transform, and load comments into dataframe.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    reddit     -- reddit instance from praw\n",
    "    subreddit  -- name of subreddit (default 'all')\n",
    "    limit_subs -- number of submissions to extract\n",
    "    depth      -- maximum traversal depth of comment tree; None returns whole tree (default None)\n",
    "    limit_coms -- number of MoreComments instances to replace (default 32)\n",
    "    threshold  -- minimum number of children required to expand MoreComments (default 0)\n",
    "    '''\n",
    "    submissions = [submission for submission in reddit.subreddit(subreddit).hot(limit=limit_subs)]\n",
    "    comments = []\n",
    "    for submission in submissions:\n",
    "        comments.extend(get_comments(submission, depth=depth, limit=limit_coms, threshold=threshold))\n",
    "    data = to_dataframe(comments)\n",
    "    return data, submissions, comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: load comments from hot submissions in /r/AskReddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9e1dd0746c07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubmissions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreddit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubreddit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'AskReddit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit_subs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ba4ca72182e5>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(reddit, subreddit, limit_subs, depth, limit_coms, threshold)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msubmission\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubmissions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mcomments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_comments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmission\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit_coms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubmissions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcomments\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-ba4ca72182e5>\u001b[0m in \u001b[0;36mto_dataframe\u001b[1;34m(comments, columns)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcomment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcomments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcomment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcomments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "data, submissions, comments = load_data(reddit, subreddit='AskReddit', limit_subs=5, depth=3)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create wordcloud from all loaded comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Andy Mueller's wordcloud (https://github.com/amueller/word_cloud)\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Join all comments into one string\n",
    "text = ' '.join(data['body'])\n",
    "\n",
    "## Generate the wordcloud\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "## Display wordcloud with matplotlib\n",
    "_ = plt.imshow(wordcloud, interpolation='bilinear')\n",
    "_ = plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
